


- "normal language model": input word and character, predict characters of next word 
- LSTM: input vector: [WORD] + last hidden state of Character LSTM
- I loop through sentence so that I can get hidden state at each time point (word)
- CharGenerator: task: predict characters of next word
	- input: concatenated hidden state with last char (not predicted word up to this point as in Matthews) 
	- last_char encoded as an embedding (with nn.Embedding())
 
- for first loop (batches): no padding needed, words are split evenly across batches 
- what about generating characters? 
	- padded up to word length with zeros: not ideal because 0 is also an index of character 
	- how to know when to stop predicting? Once <eow> was predicted? 

- word level predictor 

- questions: how to handle batches?
	- padding
	- eow/bow
		- bow: added as first character in generator
		- eow: added when creating char input 
	- end of sentence and OOV (restrict vocabulary size?)? 
	- when to stop predicting: take into account what we know about target word? 
	- zero index? 
- calculate loss? (At the moment I calculate loss after each word), summing loss over words in batch
include word level prediction?



TO DO:

Check what Matthews means by word predicted up to this point

Stop predicting: how is it done in normal language model? 


Levenshtein distance loss 

Still include word predictor loss
Include evaluation method: generate words 
Parallelise

- check parameters
-  
